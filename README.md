1.Deepseek basics 

2. LLM Architecture
   
4. Attention is all you need 

3.1. tokenziation , position encoding 

5. Self Attention 

6. Casual Attention

7. Multi head attention

8. Key value cache 

9. Multi query attention 

10. Grouped query Attention

11. Multi head latent attention

12. integer and binary positional encodings 

13. sinsoidal positional encoding 

14. Rotary position encoding 

15. Deepseek implemented latent attention

16. mixture of experts

17. deepseek rewrote moe 

18. multi -token prediction

19. deepseek rewrote multi-token prediction

20. llm quantization

21. deepseek rewrote quantization

