{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d788c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from typing import Optional,Tuple,List\n",
    "import math\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81df5491",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class CacheConfig : \n",
    "    \"\"\"config for KV cache\"\"\"\n",
    "    max_batch_size : int = 8 \n",
    "    max_seq_length : int = 2048 \n",
    "    num_layers : int = 12\n",
    "    num_heads : int = 12\n",
    "    head_dim : int = 64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48ebc161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    \"\"\"\n",
    "    Simple Key-Value cache for a single layer.\n",
    "    Stores past key and value tensors to avoid recomputation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size: int, max_seq_len: int, num_heads: int, head_dim: int, device='cuda'):\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.device = device\n",
    "        \n",
    "        # Pre-allocate cache tensors\n",
    "        self.key_cache = torch.zeros(\n",
    "            batch_size, num_heads, max_seq_len, head_dim,\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        self.value_cache = torch.zeros(\n",
    "            batch_size, num_heads, max_seq_len, head_dim,\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        self.seq_len = 0  # Current sequence length\n",
    "    \n",
    "    def update(self, key: torch.Tensor, value: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Update cache with new key and value tensors.\n",
    "        \n",
    "        Args:\n",
    "            key: New keys [batch_size, num_heads, new_seq_len, head_dim]\n",
    "            value: New values [batch_size, num_heads, new_seq_len, head_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Complete key and value tensors including cached history\n",
    "        \"\"\"\n",
    "        batch_size, num_heads, new_seq_len, head_dim = key.shape\n",
    "        \n",
    "        # Store new keys and values in cache\n",
    "        self.key_cache[:batch_size, :, self.seq_len:self.seq_len + new_seq_len] = key\n",
    "        self.value_cache[:batch_size, :, self.seq_len:self.seq_len + new_seq_len] = value\n",
    "        \n",
    "        # Update sequence length\n",
    "        self.seq_len += new_seq_len\n",
    "        \n",
    "        # Return full cache up to current position\n",
    "        full_keys = self.key_cache[:batch_size, :, :self.seq_len]\n",
    "        full_values = self.value_cache[:batch_size, :, :self.seq_len]\n",
    "        \n",
    "        return full_keys, full_values\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset cache to empty state\"\"\"\n",
    "        self.seq_len = 0\n",
    "        self.key_cache.zero_()\n",
    "        self.value_cache.zero_()\n",
    "    \n",
    "    def get_seq_length(self) -> int:\n",
    "        \"\"\"Get current sequence length in cache\"\"\"\n",
    "        return self.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a72ee928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithCache(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention with KV cache support.\n",
    "    Demonstrates how to use KV cache in practice.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        # Linear projections\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        \"\"\"\n",
    "        Forward pass with optional KV cache.\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: Input tensor [batch_size, seq_len, d_model]\n",
    "            attention_mask: Mask tensor [batch_size, 1, seq_len, cached_seq_len]\n",
    "            kv_cache: Optional KV cache from previous steps\n",
    "            use_cache: Whether to return updated cache\n",
    "            \n",
    "        Returns:\n",
    "            output: Attention output [batch_size, seq_len, d_model]\n",
    "            kv_cache: Updated cache if use_cache=True\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        \n",
    "        # Project queries, keys, values\n",
    "        queries = self.q_proj(hidden_states)\n",
    "        keys = self.k_proj(hidden_states)\n",
    "        values = self.v_proj(hidden_states)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # [batch_size, seq_len, d_model] -> [batch_size, num_heads, seq_len, head_dim]\n",
    "        queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Update cache if provided\n",
    "        if kv_cache is not None:\n",
    "            keys, values = kv_cache.update(keys, values)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if attention_mask is not None:\n",
    "            attn_scores = attn_scores + attention_mask\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, values)\n",
    "        \n",
    "        # Reshape back\n",
    "        # [batch_size, num_heads, seq_len, head_dim] -> [batch_size, seq_len, d_model]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output, kv_cache if use_cache else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0654e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayerWithCache(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer layer with KV cache support.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttentionWithCache(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "        use_cache: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        # Self-attention with residual\n",
    "        attn_output, kv_cache = self.attention(\n",
    "            self.norm1(hidden_states),\n",
    "            attention_mask=attention_mask,\n",
    "            kv_cache=kv_cache,\n",
    "            use_cache=use_cache\n",
    "        )\n",
    "        hidden_states = hidden_states + attn_output\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        hidden_states = hidden_states + self.ffn(self.norm2(hidden_states))\n",
    "        \n",
    "        return hidden_states, kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "874fa902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len: int, cached_len: int = 0, device='cuda') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create causal attention mask for autoregressive generation.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Current sequence length\n",
    "        cached_len: Length of cached sequence\n",
    "        device: Device to create mask on\n",
    "        \n",
    "    Returns:\n",
    "        Causal mask tensor\n",
    "    \"\"\"\n",
    "    total_len = seq_len + cached_len\n",
    "    mask = torch.triu(torch.ones(seq_len, total_len, device=device), diagonal=cached_len + 1)\n",
    "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "    return mask.unsqueeze(0).unsqueeze(0)  # Add batch and head dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29327624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "batch_size = 2\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "head_dim = d_model // num_heads\n",
    "max_seq_len = 100\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5396560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = TransformerLayerWithCache(d_model, num_heads, d_ff=2048).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c9e7fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLayerWithCache(\n",
       "  (attention): MultiHeadAttentionWithCache(\n",
       "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (ffn): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (4): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.eval()  # Set to eval mode for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "97c3fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_len = 10\n",
    "prompt = torch.randn(batch_size, prompt_len, d_model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "21b6b42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28be04a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Current Seq Length: 11\n",
      "Step 6, Current Seq Length: 16\n",
      "Step 11, Current Seq Length: 21\n",
      "Step 16, Current Seq Length: 26\n",
      "Inference completed.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    current_seq = prompt.clone()  # Start with prompt as current sequence\n",
    "    for step in range(20):  # Simulate generating 20 new tokens\n",
    "        # need to process entire sequence every time \n",
    "        seq_len = current_seq.shape[1]\n",
    "        mask = create_causal_mask(seq_len, 0,device=device)\n",
    "        output, _ = layer(current_seq, attention_mask=mask,  use_cache=False)\n",
    "\n",
    "        #simulate th getting next token \n",
    "        next_token = torch.randn(batch_size, 1, d_model, device=device)\n",
    "        current_seq = torch.cat([current_seq, next_token], dim=1)\n",
    "\n",
    "        if step % 5 == 0:\n",
    "            print(f\"Step {step+1}, Current Seq Length: {current_seq.shape[1]}\")\n",
    "\n",
    "print(\"Inference completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45e7f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed prompt: 10 tokens, cache length: 10\n",
      "  Step 0: Processing 1 token, cache length: 11\n",
      "  Step 5: Processing 1 token, cache length: 16\n",
      "  Step 10: Processing 1 token, cache length: 21\n",
      "  Step 15: Processing 1 token, cache length: 26\n",
      "Final cache length: 30\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        # Initialize cache\n",
    "        kv_cache = KVCache(batch_size, max_seq_len, num_heads, head_dim, device)\n",
    "        \n",
    "        # Process initial prompt\n",
    "        mask = create_causal_mask(prompt_len, 0, device)\n",
    "        output, kv_cache = layer(prompt, attention_mask=mask, kv_cache=kv_cache, use_cache=True)\n",
    "        print(f\"  Processed prompt: {prompt_len} tokens, cache length: {kv_cache.get_seq_length()}\")\n",
    "        \n",
    "        current_token = torch.randn(batch_size, 1, d_model, device=device)\n",
    "        \n",
    "        for step in range(20):\n",
    "            # Only process 1 new token at a time!\n",
    "            cached_len = kv_cache.get_seq_length()\n",
    "            mask = create_causal_mask(1, cached_len, device)\n",
    "            \n",
    "            output, kv_cache = layer(\n",
    "                current_token,\n",
    "                attention_mask=mask,\n",
    "                kv_cache=kv_cache,\n",
    "                use_cache=True\n",
    "            )\n",
    "            \n",
    "            # Simulate getting next token\n",
    "            current_token = torch.randn(batch_size, 1, d_model, device=device)\n",
    "            \n",
    "            if step % 5 == 0:\n",
    "                print(f\"  Step {step}: Processing 1 token, cache length: {kv_cache.get_seq_length()}\")\n",
    "    \n",
    "print(f\"Final cache length: {kv_cache.get_seq_length()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f908697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
