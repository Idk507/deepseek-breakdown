{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c0e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "from typing import Optional, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c07d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. BASIC GROUPED QUERY ATTENTION\n",
    "# ============================================================================\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped Query Attention (GQA) implementation.\n",
    "    \n",
    "    GQA shares keys and values across groups of query heads,\n",
    "    reducing memory usage while maintaining quality.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_query_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: Model dimension\n",
    "            num_query_heads: Number of query heads\n",
    "            num_kv_heads: Number of key-value heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(GroupedQueryAttention, self).__init__()\n",
    "        \n",
    "        assert embed_dim % num_query_heads == 0, \\\n",
    "            \"embed_dim must be divisible by num_query_heads\"\n",
    "        assert num_query_heads % num_kv_heads == 0, \\\n",
    "            \"num_query_heads must be divisible by num_kv_heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = embed_dim // num_query_heads\n",
    "        self.group_size = num_query_heads // num_kv_heads\n",
    "        \n",
    "        # Query projection (one per query head)\n",
    "        self.q_proj = nn.Linear(embed_dim, num_query_heads * self.head_dim)\n",
    "        \n",
    "        # Key-Value projections (one per KV head)\n",
    "        self.k_proj = nn.Linear(embed_dim, num_kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, num_kv_heads * self.head_dim)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        return_attention: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor [batch, seq_len, embed_dim]\n",
    "            mask: Optional attention mask\n",
    "            return_attention: Whether to return attention weights\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch, seq_len, embed_dim]\n",
    "            attention_weights: [batch, num_query_heads, seq_len, seq_len] (optional)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        # Q: [batch, seq_len, num_query_heads * head_dim]\n",
    "        Q = self.q_proj(x)\n",
    "        # K, V: [batch, seq_len, num_kv_heads * head_dim]\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # Q: [batch, seq_len, num_query_heads, head_dim]\n",
    "        # -> [batch, num_query_heads, seq_len, head_dim]\n",
    "        Q = Q.view(batch_size, seq_len, self.num_query_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # K, V: [batch, seq_len, num_kv_heads, head_dim]\n",
    "        # -> [batch, num_kv_heads, seq_len, head_dim]\n",
    "        K = K.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Expand K, V to match number of query heads\n",
    "        # Method: Repeat each KV head group_size times\n",
    "        # [batch, num_kv_heads, seq_len, head_dim]\n",
    "        # -> [batch, num_kv_heads, 1, seq_len, head_dim]\n",
    "        # -> [batch, num_kv_heads, group_size, seq_len, head_dim]\n",
    "        # -> [batch, num_query_heads, seq_len, head_dim]\n",
    "        K = K.unsqueeze(2).expand(\n",
    "            batch_size, self.num_kv_heads, self.group_size, seq_len, self.head_dim\n",
    "        ).reshape(batch_size, self.num_query_heads, seq_len, self.head_dim)\n",
    "        \n",
    "        V = V.unsqueeze(2).expand(\n",
    "            batch_size, self.num_kv_heads, self.group_size, seq_len, self.head_dim\n",
    "        ).reshape(batch_size, self.num_query_heads, seq_len, self.head_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # [batch, num_query_heads, seq_len, head_dim] @ [batch, num_query_heads, head_dim, seq_len]\n",
    "        # -> [batch, num_query_heads, seq_len, seq_len]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        # [batch, num_query_heads, seq_len, seq_len] @ [batch, num_query_heads, seq_len, head_dim]\n",
    "        # -> [batch, num_query_heads, seq_len, head_dim]\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape back\n",
    "        # [batch, num_query_heads, seq_len, head_dim]\n",
    "        # -> [batch, seq_len, num_query_heads, head_dim]\n",
    "        # -> [batch, seq_len, embed_dim]\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.embed_dim\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attn_weights\n",
    "        return output, None\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. OPTIMIZED GQA WITH KV CACHE\n",
    "# ============================================================================\n",
    "\n",
    "class CachedGroupedQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped Query Attention with KV caching for efficient autoregressive generation.\n",
    "    \n",
    "    During generation, we cache computed keys and values to avoid redundant\n",
    "    computation. GQA makes this cache much smaller than standard MHA.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_query_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super(CachedGroupedQueryAttention, self).__init__()\n",
    "        \n",
    "        assert embed_dim % num_query_heads == 0\n",
    "        assert num_query_heads % num_kv_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_query_heads = num_query_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = embed_dim // num_query_heads\n",
    "        self.group_size = num_query_heads // num_kv_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_dim, num_query_heads * self.head_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, num_kv_heads * self.head_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, num_kv_heads * self.head_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "        use_cache: bool = False,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input [batch, seq_len, embed_dim]\n",
    "            past_kv: Cached (K, V) from previous steps\n",
    "            use_cache: Whether to return updated cache\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch, seq_len, embed_dim]\n",
    "            present_kv: Updated cache (if use_cache=True)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Project queries\n",
    "        Q = self.q_proj(x)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_query_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Project keys and values\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        K = K.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_kv_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Concatenate with past KV if provided\n",
    "        if past_kv is not None:\n",
    "            past_K, past_V = past_kv\n",
    "            K = torch.cat([past_K, K], dim=2)  # Concatenate along sequence dimension\n",
    "            V = torch.cat([past_V, V], dim=2)\n",
    "        \n",
    "        # Store for next step if caching\n",
    "        present_kv = (K, V) if use_cache else None\n",
    "        \n",
    "        # Expand K, V for grouped attention\n",
    "        total_seq_len = K.size(2)\n",
    "        K_expanded = K.unsqueeze(2).expand(\n",
    "            batch_size, self.num_kv_heads, self.group_size, total_seq_len, self.head_dim\n",
    "        ).reshape(batch_size, self.num_query_heads, total_seq_len, self.head_dim)\n",
    "        \n",
    "        V_expanded = V.unsqueeze(2).expand(\n",
    "            batch_size, self.num_kv_heads, self.group_size, total_seq_len, self.head_dim\n",
    "        ).reshape(batch_size, self.num_query_heads, total_seq_len, self.head_dim)\n",
    "        \n",
    "        # Compute attention\n",
    "        scores = torch.matmul(Q, K_expanded.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights, V_expanded)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.embed_dim\n",
    "        )\n",
    "        \n",
    "        output = self.out_proj(attn_output)\n",
    "        \n",
    "        return output, present_kv\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. CONVERSION UTILITIES: MHA → GQA\n",
    "# ============================================================================\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Standard Multi-Head Attention for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        Q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        output = torch.matmul(attn, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def convert_mha_to_gqa(\n",
    "    mha_module: MultiHeadAttention,\n",
    "    num_kv_heads: int,\n",
    "    strategy: str = 'mean'\n",
    ") -> GroupedQueryAttention:\n",
    "    \"\"\"\n",
    "    Convert a trained Multi-Head Attention to Grouped Query Attention.\n",
    "    \n",
    "    Args:\n",
    "        mha_module: Trained MHA module\n",
    "        num_kv_heads: Number of KV heads in GQA\n",
    "        strategy: 'mean' or 'first' - how to combine heads\n",
    "    \n",
    "    Returns:\n",
    "        gqa_module: Converted GQA module\n",
    "    \"\"\"\n",
    "    assert mha_module.num_heads % num_kv_heads == 0, \\\n",
    "        \"num_heads must be divisible by num_kv_heads\"\n",
    "    \n",
    "    group_size = mha_module.num_heads // num_kv_heads\n",
    "    \n",
    "    # Create GQA module\n",
    "    gqa = GroupedQueryAttention(\n",
    "        embed_dim=mha_module.embed_dim,\n",
    "        num_query_heads=mha_module.num_heads,\n",
    "        num_kv_heads=num_kv_heads,\n",
    "        dropout=0.0  # Will copy from MHA\n",
    "    )\n",
    "    \n",
    "    # Copy query projection (unchanged)\n",
    "    gqa.q_proj.weight.data = mha_module.q_proj.weight.data.clone()\n",
    "    gqa.q_proj.bias.data = mha_module.q_proj.bias.data.clone()\n",
    "    \n",
    "    # Copy output projection (unchanged)\n",
    "    gqa.out_proj.weight.data = mha_module.out_proj.weight.data.clone()\n",
    "    gqa.out_proj.bias.data = mha_module.out_proj.bias.data.clone()\n",
    "    \n",
    "    # Convert K, V projections\n",
    "    head_dim = mha_module.head_dim\n",
    "    \n",
    "    # Reshape MHA K, V weights to [num_heads, head_dim, embed_dim]\n",
    "    k_weight = mha_module.k_proj.weight.data.view(\n",
    "        mha_module.num_heads, head_dim, mha_module.embed_dim\n",
    "    )\n",
    "    v_weight = mha_module.v_proj.weight.data.view(\n",
    "        mha_module.num_heads, head_dim, mha_module.embed_dim\n",
    "    )\n",
    "    \n",
    "    k_bias = mha_module.k_proj.bias.data.view(mha_module.num_heads, head_dim)\n",
    "    v_bias = mha_module.v_proj.bias.data.view(mha_module.num_heads, head_dim)\n",
    "    \n",
    "    # Group and combine\n",
    "    new_k_weight = []\n",
    "    new_v_weight = []\n",
    "    new_k_bias = []\n",
    "    new_v_bias = []\n",
    "    \n",
    "    for g in range(num_kv_heads):\n",
    "        start_idx = g * group_size\n",
    "        end_idx = start_idx + group_size\n",
    "        \n",
    "        if strategy == 'mean':\n",
    "            # Average weights across group\n",
    "            k_g = k_weight[start_idx:end_idx].mean(dim=0)\n",
    "            v_g = v_weight[start_idx:end_idx].mean(dim=0)\n",
    "            kb_g = k_bias[start_idx:end_idx].mean(dim=0)\n",
    "            vb_g = v_bias[start_idx:end_idx].mean(dim=0)\n",
    "        elif strategy == 'first':\n",
    "            # Take first head in group\n",
    "            k_g = k_weight[start_idx]\n",
    "            v_g = v_weight[start_idx]\n",
    "            kb_g = k_bias[start_idx]\n",
    "            vb_g = v_bias[start_idx]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "        \n",
    "        new_k_weight.append(k_g)\n",
    "        new_v_weight.append(v_g)\n",
    "        new_k_bias.append(kb_g)\n",
    "        new_v_bias.append(vb_g)\n",
    "    \n",
    "    # Stack and reshape\n",
    "    new_k_weight = torch.stack(new_k_weight).view(num_kv_heads * head_dim, mha_module.embed_dim)\n",
    "    new_v_weight = torch.stack(new_v_weight).view(num_kv_heads * head_dim, mha_module.embed_dim)\n",
    "    new_k_bias = torch.stack(new_k_bias).view(num_kv_heads * head_dim)\n",
    "    new_v_bias = torch.stack(new_v_bias).view(num_kv_heads * head_dim)\n",
    "    \n",
    "    # Assign to GQA\n",
    "    gqa.k_proj.weight.data = new_k_weight\n",
    "    gqa.v_proj.weight.data = new_v_weight\n",
    "    gqa.k_proj.bias.data = new_k_bias\n",
    "    gqa.v_proj.bias.data = new_v_bias\n",
    "    \n",
    "    return gqa\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. COMPLETE MODEL EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "class GQATransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer block with GQA.\n",
    "    Similar to GPT architecture but with grouped query attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_query_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        ff_dim: Optional[int] = None,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super(GQATransformerBlock, self).__init__()\n",
    "        \n",
    "        if ff_dim is None:\n",
    "            ff_dim = 4 * embed_dim\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.attn = GroupedQueryAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_query_heads=num_query_heads,\n",
    "            num_kv_heads=num_kv_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Attention with residual\n",
    "        attn_out, _ = self.attn(self.ln1(x), mask=mask)\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # FFN with residual\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. PERFORMANCE BENCHMARKING\n",
    "# ============================================================================\n",
    "\n",
    "class AttentionBenchmark:\n",
    "    \"\"\"Benchmark different attention mechanisms.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def benchmark_memory(\n",
    "        batch_size: int,\n",
    "        seq_len: int,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        num_kv_heads: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Measure memory usage of MHA vs GQA.\n",
    "        \n",
    "        Returns:\n",
    "            dict with memory statistics\n",
    "        \"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Create input\n",
    "        x = torch.randn(batch_size, seq_len, embed_dim, device=device)\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        mha = MultiHeadAttention(embed_dim, num_heads).to(device)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        _ = mha(x)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "            mha_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "        else:\n",
    "            mha_memory = 0\n",
    "        \n",
    "        # Grouped Query Attention\n",
    "        gqa = GroupedQueryAttention(embed_dim, num_heads, num_kv_heads).to(device)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        _ = gqa(x)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "            gqa_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "        else:\n",
    "            gqa_memory = 0\n",
    "        \n",
    "        return {\n",
    "            'mha_memory_mb': mha_memory,\n",
    "            'gqa_memory_mb': gqa_memory,\n",
    "            'reduction': (mha_memory - gqa_memory) / mha_memory if mha_memory > 0 else 0\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def benchmark_speed(\n",
    "        batch_size: int,\n",
    "        seq_len: int,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        num_iterations: int = 100\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Measure inference speed of MHA vs GQA.\n",
    "        \n",
    "        Returns:\n",
    "            dict with timing statistics\n",
    "        \"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        x = torch.randn(batch_size, seq_len, embed_dim, device=device)\n",
    "        \n",
    "        # MHA\n",
    "        mha = MultiHeadAttention(embed_dim, num_heads).to(device)\n",
    "        mha.eval()\n",
    "        \n",
    "        # Warmup\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):\n",
    "                _ = mha(x)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_iterations):\n",
    "                _ = mha(x)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        mha_time = (time.time() - start) / num_iterations\n",
    "        \n",
    "        # GQA\n",
    "        gqa = GroupedQueryAttention(embed_dim, num_heads, num_kv_heads).to(device)\n",
    "        gqa.eval()\n",
    "        \n",
    "        # Warmup\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):\n",
    "                _ = gqa(x)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(num_iterations):\n",
    "                _ = gqa(x)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        gqa_time = (time.time() - start) / num_iterations\n",
    "        \n",
    "        return {\n",
    "            'mha_time_ms': mha_time * 1000,\n",
    "            'gqa_time_ms': gqa_time * 1000,\n",
    "            'speedup': mha_time / gqa_time\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def benchmark_kv_cache_size(\n",
    "        seq_len: int,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        num_kv_heads: int,\n",
    "        num_layers: int = 32\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculate KV cache size for different attention mechanisms.\n",
    "        \n",
    "        Returns:\n",
    "            dict with cache size information\n",
    "        \"\"\"\n",
    "        head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # MHA cache size\n",
    "        mha_cache = 2 * num_layers * num_heads * head_dim * seq_len * 2  # 2 bytes for FP16\n",
    "        \n",
    "        # GQA cache size\n",
    "        gqa_cache = 2 * num_layers * num_kv_heads * head_dim * seq_len * 2\n",
    "        \n",
    "        return {\n",
    "            'mha_cache_mb': mha_cache / (1024**2),\n",
    "            'gqa_cache_mb': gqa_cache / (1024**2),\n",
    "            'reduction_factor': num_heads / num_kv_heads,\n",
    "            'memory_saved_mb': (mha_cache - gqa_cache) / (1024**2)\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. DEMONSTRATION EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "def demo_basic_gqa():\n",
    "    \"\"\"Demonstrate basic GQA usage.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"DEMO 1: Basic Grouped Query Attention\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    embed_dim = 256\n",
    "    num_query_heads = 16\n",
    "    num_kv_heads = 4\n",
    "    \n",
    "    # Create GQA layer\n",
    "    gqa = GroupedQueryAttention(embed_dim, num_query_heads, num_kv_heads)\n",
    "    \n",
    "    # Create input\n",
    "    x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, attn_weights = gqa(x, return_attention=True)\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Input shape: {x.shape}\")\n",
    "    print(f\"  Query heads: {num_query_heads}\")\n",
    "    print(f\"  KV heads: {num_kv_heads}\")\n",
    "    print(f\"  Group size: {num_query_heads // num_kv_heads}\")\n",
    "    print(f\"  Head dimension: {embed_dim // num_query_heads}\")\n",
    "    \n",
    "    print(f\"\\nOutput:\")\n",
    "    print(f\"  Shape: {output.shape}\")\n",
    "    print(f\"  Attention weights shape: {attn_weights.shape}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    mha_params = 4 * embed_dim * embed_dim  # Q, K, V, O for MHA\n",
    "    gqa_params = sum(p.numel() for p in gqa.parameters())\n",
    "    \n",
    "    print(f\"\\nParameters:\")\n",
    "    print(f\"  GQA: {gqa_params:,}\")\n",
    "    print(f\"  MHA (equivalent): {mha_params:,}\")\n",
    "    print(f\"  Reduction: {(1 - gqa_params/mha_params)*100:.1f}%\")\n",
    "\n",
    "\n",
    "def demo_mha_to_gqa_conversion():\n",
    "    \"\"\"Demonstrate converting MHA to GQA.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEMO 2: MHA to GQA Conversion\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    embed_dim = 128\n",
    "    num_heads = 8\n",
    "    num_kv_heads = 4\n",
    "    \n",
    "    # Create and \"train\" MHA\n",
    "    mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "    \n",
    "    # Simulate some training (just for demo)\n",
    "    x = torch.randn(4, 20, embed_dim)\n",
    "    _ = mha(x)\n",
    "    \n",
    "    print(f\"\\nOriginal MHA:\")\n",
    "    print(f\"  Num heads: {num_heads}\")\n",
    "    mha_params = sum(p.numel() for p in mha.parameters())\n",
    "    print(f\"  Parameters: {mha_params:,}\")\n",
    "    \n",
    "    # Convert to GQA\n",
    "    gqa = convert_mha_to_gqa(mha, num_kv_heads, strategy='mean')\n",
    "    \n",
    "    print(f\"\\nConverted GQA:\")\n",
    "    print(f\"  Query heads: {gqa.num_query_heads}\")\n",
    "    print(f\"  KV heads: {gqa.num_kv_heads}\")\n",
    "    gqa_params = sum(p.numel() for p in gqa.parameters())\n",
    "    print(f\"  Parameters: {gqa_params:,}\")\n",
    "    print(f\"  Reduction: {(1 - gqa_params/mha_params)*100:.1f}%\")\n",
    "    \n",
    "    # Test that conversion works\n",
    "    with torch.no_grad():\n",
    "        mha_out = mha(x)\n",
    "        gqa_out, _ = gqa(x)\n",
    "        \n",
    "        # They won't be identical due to grouping, but should be similar\n",
    "        diff = (mha_out - gqa_out).abs().mean()\n",
    "        print(f\"\\nOutput difference: {diff:.6f}\")\n",
    "        print(f\"  (Non-zero expected due to weight averaging)\")\n",
    "\n",
    "\n",
    "def demo_kv_cache():\n",
    "    \"\"\"Demonstrate KV caching with GQA.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEMO 3: KV Caching with GQA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    embed_dim = 128\n",
    "    num_query_heads = 8\n",
    "    num_kv_heads = 4\n",
    "    \n",
    "    gqa = CachedGroupedQueryAttention(embed_dim, num_query_heads, num_kv_heads)\n",
    "    gqa.eval()\n",
    "    \n",
    "    # Simulate autoregressive generation\n",
    "    print(\"\\nSimulating token-by-token generation:\")\n",
    "    \n",
    "    # Initial token\n",
    "    x = torch.randn(1, 1, embed_dim)\n",
    "    print(f\"\\nStep 1: Process token 1\")\n",
    "    print(f\"  Input shape: {x.shape}\")\n",
    "    \n",
    "    output, kv_cache = gqa(x, past_kv=None, use_cache=True)\n",
    "    print(f\"  KV cache created: K shape {kv_cache[0].shape}, V shape {kv_cache[1].shape}\")\n",
    "    \n",
    "    # Next tokens using cache\n",
    "    for step in range(2, 5):\n",
    "        x = torch.randn(1, 1, embed_dim)\n",
    "        output, kv_cache = gqa(x, past_kv=kv_cache, use_cache=True)\n",
    "        print(f\"\\nStep {step}: Process token {step}\")\n",
    "        print(f\"  Input shape: {x.shape}\")\n",
    "        print(f\"  KV cache grows: K shape {kv_cache[0].shape}\")\n",
    "    \n",
    "    print(f\"\\nFinal cache contains {kv_cache[0].size(2)} tokens\")\n",
    "    print(f\"Only computed new K, V for each new token (not recomputed past)\")\n",
    "\n",
    "\n",
    "def demo_performance_comparison():\n",
    "    \"\"\"Compare performance of different attention mechanisms.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEMO 4: Performance Comparison\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    configs = [\n",
    "        (4, 512, 256, 16, 16, \"MHA (baseline)\"),\n",
    "        (4, 512, 256, 16, 8, \"GQA (2:1)\"),\n",
    "        (4, 512, 256, 16, 4, \"GQA (4:1)\"),\n",
    "        (4, 512, 256, 16, 2, \"GQA (8:1)\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nSpeed Benchmark:\")\n",
    "    print(f\"{'Config':<20} {'Time (ms)':<12} {'Speedup':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    baseline_time = None\n",
    "    for batch, seq, dim, qh, kvh, name in configs:\n",
    "        result = AttentionBenchmark.benchmark_speed(\n",
    "            batch, seq, dim, qh, kvh, num_iterations=50\n",
    "        )\n",
    "        time_ms = result['gqa_time_ms'] if kvh < qh else result['mha_time_ms']\n",
    "        \n",
    "        if baseline_time is None:\n",
    "            baseline_time = time_ms\n",
    "            speedup = 1.0\n",
    "        else:\n",
    "            speedup = baseline_time / time_ms\n",
    "        \n",
    "        print(f\"{name:<20} {time_ms:>10.2f}  {speedup:>8.2f}x\")\n",
    "    \n",
    "    print(\"\\nKV Cache Size Comparison:\")\n",
    "    print(f\"{'Config':<20} {'Cache (MB)':<12} {'Reduction':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for batch, seq, dim, qh, kvh, name in configs:\n",
    "        result = AttentionBenchmark.benchmark_kv_cache_size(\n",
    "            seq_len=seq, embed_dim=dim, num_heads=qh, \n",
    "            num_kv_heads=kvh, num_layers=32\n",
    "        )\n",
    "        cache_mb = result['gqa_cache_mb'] if kvh < qh else result['mha_cache_mb']\n",
    "        reduction = f\"{qh/kvh:.0f}x\" if kvh < qh else \"-\"\n",
    "        \n",
    "        print(f\"{name:<20} {cache_mb:>10.1f}  {reduction:>10}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GROUPED QUERY ATTENTION: COMPLETE IMPLEMENTATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Run all demos\n",
    "    demo_basic_gqa()\n",
    "    demo_mha_to_gqa_conversion()\n",
    "    demo_kv_cache()\n",
    "    demo_performance_comparison()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"All demonstrations completed successfully!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ADDITIONAL UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_grouping_pattern(num_query_heads: int, num_kv_heads: int):\n",
    "    \"\"\"\n",
    "    Visualize which query heads share which KV heads.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    group_size = num_query_heads // num_kv_heads\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Draw query heads\n",
    "    for i in range(num_query_heads):\n",
    "        color_idx = i // group_size\n",
    "        color = plt.cm.tab10(color_idx % 10)\n",
    "        rect = patches.Rectangle((i, 2), 0.8, 0.8, \n",
    "                                 linewidth=2, edgecolor='black', \n",
    "                                 facecolor=color, alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(i + 0.4, 2.4, f'Q{i}', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Draw KV heads\n",
    "    for j in range(num_kv_heads):\n",
    "        color = plt.cm.tab10(j % 10)\n",
    "        rect = patches.Rectangle((j * group_size + group_size/2 - 0.4, 0.5), \n",
    "                                 0.8, 0.8,\n",
    "                                 linewidth=2, edgecolor='black', \n",
    "                                 facecolor=color, alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(j * group_size + group_size/2, 0.9, f'KV{j}', \n",
    "               ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw connections\n",
    "    for i in range(num_query_heads):\n",
    "        kv_idx = i // group_size\n",
    "        x_start = i + 0.4\n",
    "        x_end = kv_idx * group_size + group_size/2\n",
    "        ax.plot([x_start, x_end], [2, 1.3], 'k--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    ax.set_xlim(-0.5, num_query_heads + 0.5)\n",
    "    ax.set_ylim(0, 3.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax.text(num_query_heads/2, 3.2, \n",
    "           f'Grouped Query Attention: {num_query_heads} Query Heads, {num_kv_heads} KV Heads',\n",
    "           ha='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/tmp/gqa_grouping.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"Grouping visualization saved to /tmp/gqa_grouping.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_grouping_pattern(num_query_heads=8, num_kv_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GQA layer\n",
    "gqa = GroupedQueryAttention(\n",
    "    embed_dim=512,\n",
    "    num_query_heads=32,\n",
    "    num_kv_heads=8  # 4× memory reduction\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(16, 100, 512)  # [batch, seq, dim]\n",
    "output, _ = gqa(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5817c0",
   "metadata": {},
   "source": [
    "With KV Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29382e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "gqa = CachedGroupedQueryAttention(\n",
    "    embed_dim=512,\n",
    "    num_query_heads=32,\n",
    "    num_kv_heads=8\n",
    ")\n",
    "\n",
    "# First token\n",
    "output, cache = gqa(x[:, 0:1], use_cache=True)\n",
    "\n",
    "# Subsequent tokens (much faster!)\n",
    "for i in range(1, seq_len):\n",
    "    output, cache = gqa(x[:, i:i+1], past_kv=cache, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927eab97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58c64933",
   "metadata": {},
   "source": [
    "Convert Existing MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient expansion using broadcasting\n",
    "K = K.unsqueeze(2).expand(\n",
    "    batch, num_kv_heads, group_size, seq_len, head_dim\n",
    ").reshape(batch, num_query_heads, seq_len, head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a7a94",
   "metadata": {},
   "source": [
    " K, V Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15212f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient expansion using broadcasting\n",
    "K = K.unsqueeze(2).expand(\n",
    "    batch, num_kv_heads, group_size, seq_len, head_dim\n",
    ").reshape(batch, num_query_heads, seq_len, head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6863d32",
   "metadata": {},
   "source": [
    "Memory Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store KV cache efficiently\n",
    "# Shape: [batch, num_kv_heads, seq_len, head_dim]\n",
    "# Not: [batch, num_query_heads, seq_len, head_dim]\n",
    "# Saves (num_query_heads / num_kv_heads)× memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7c8211",
   "metadata": {},
   "source": [
    "Group Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query head i uses KV head j\n",
    "j = i // group_size\n",
    "\n",
    "# Example: 8 query heads, 4 KV heads\n",
    "# Q0, Q1 → KV0\n",
    "# Q2, Q3 → KV1\n",
    "# Q4, Q5 → KV2\n",
    "# Q6, Q7 → KV3"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
