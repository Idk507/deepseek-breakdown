{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Tuple, Optional, List\n",
    "import numpy as n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expert method \n",
    "\n",
    "class SwiGLUExpert(nn.Module):\n",
    "    \"\"\" Single Expert Using SwiGLU activation function. \n",
    "        \n",
    "    SwiGLU(x) = (x W_gate ⊙ σ(x W_up)) W_down\n",
    "    where σ is SiLU activation, ⊙ is element-wise product\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim : int, hidden_dim : int , dropout :float = 0.0):\n",
    "        super(SwiGLUExpert,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        #gate and up projection\n",
    "        self.gate_proj = nn.Linear(input_dim, hidden_dim,bias=False)\n",
    "        self.up_proj = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(hidden_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize weights using Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.gate_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.up_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.down_proj.weight)\n",
    "    \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        gate = self.gate_proj(x)\n",
    "        up = self.up_proj(x)\n",
    "        swiglu = F.silu(gate) * up\n",
    "        output = self.down_proj(swiglu)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f3bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# router implementation\n",
    "\n",
    "class TopKRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Router that selects top-K experts for each token.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_experts: int,\n",
    "        top_k: int = 6,\n",
    "        use_bias: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Input dimension\n",
    "            num_experts: Total number of experts\n",
    "            top_k: Number of experts to select per token\n",
    "            use_bias: Whether to use bias in router (V3 uses this)\n",
    "        \"\"\"\n",
    "        super(TopKRouter, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Router linear layer\n",
    "        self.gate = nn.Linear(input_dim, num_experts, bias=use_bias)\n",
    "        \n",
    "        nn.init.normal_(self.gate.weight, std=0.02)\n",
    "        if use_bias:\n",
    "            nn.init.zeros_(self.gate.bias)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        training: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Route tokens to experts.\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, input_dim]\n",
    "            training: Whether in training mode (for load balancing)\n",
    "        \n",
    "        Returns:\n",
    "            expert_indices: [batch_size, seq_len, top_k]\n",
    "            expert_weights: [batch_size, seq_len, top_k]\n",
    "            router_logits: [batch_size, seq_len, num_experts]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        \n",
    "        # Compute router logits\n",
    "        router_logits = self.gate(hidden_states)  # [batch, seq, num_experts]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        \n",
    "        # Select top-K experts\n",
    "        expert_weights, expert_indices = torch.topk(\n",
    "            router_probs,\n",
    "            self.top_k,\n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        # Renormalize weights\n",
    "        expert_weights = expert_weights / expert_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return expert_indices, expert_weights, router_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93899feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. DEEPSEEK-V2 MOE LAYER\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class DeepSeekV2MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepSeek-V2 MoE layer with:\n",
    "    - Fine-grained routed experts (64)\n",
    "    - Shared experts (2)\n",
    "    - Top-K routing (K=6)\n",
    "    - Auxiliary losses for load balancing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 5120,\n",
    "        num_routed_experts: int = 64,\n",
    "        num_shared_experts: int = 2,\n",
    "        expert_hidden_dim: int = 1536,\n",
    "        top_k: int = 6,\n",
    "        dropout: float = 0.0,\n",
    "        aux_loss_alpha: float = 0.01,\n",
    "        aux_loss_beta: float = 0.01\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Model dimension\n",
    "            num_routed_experts: Number of routed experts\n",
    "            num_shared_experts: Number of shared (always-active) experts\n",
    "            expert_hidden_dim: Hidden dimension for each expert\n",
    "            top_k: Number of experts to activate per token\n",
    "            dropout: Dropout probability\n",
    "            aux_loss_alpha: Weight for load balancing loss\n",
    "            aux_loss_beta: Weight for importance loss\n",
    "        \"\"\"\n",
    "        super(DeepSeekV2MoE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_routed_experts = num_routed_experts\n",
    "        self.num_shared_experts = num_shared_experts\n",
    "        self.expert_hidden_dim = expert_hidden_dim\n",
    "        self.top_k = top_k\n",
    "        self.aux_loss_alpha = aux_loss_alpha\n",
    "        self.aux_loss_beta = aux_loss_beta\n",
    "        \n",
    "        # Router for routed experts\n",
    "        self.router = TopKRouter(\n",
    "            input_dim=input_dim,\n",
    "            num_experts=num_routed_experts,\n",
    "            top_k=top_k,\n",
    "            use_bias=False\n",
    "        )\n",
    "        \n",
    "        # Routed experts\n",
    "        self.routed_experts = nn.ModuleList([\n",
    "            SwiGLUExpert(input_dim, expert_hidden_dim, dropout)\n",
    "            for _ in range(num_routed_experts)\n",
    "        ])\n",
    "        \n",
    "        # Shared experts (always active)\n",
    "        self.shared_experts = nn.ModuleList([\n",
    "            SwiGLUExpert(input_dim, expert_hidden_dim, dropout)\n",
    "            for _ in range(num_shared_experts)\n",
    "        ])\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        return_aux_loss: bool = True\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass through MoE layer.\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, input_dim]\n",
    "            return_aux_loss: Whether to compute auxiliary losses\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, input_dim]\n",
    "            aux_loss: Auxiliary loss for load balancing (if return_aux_loss)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, input_dim = hidden_states.size()\n",
    "        \n",
    "        # ================================================================\n",
    "        # 1. Shared Expert Processing (always active)\n",
    "        # ================================================================\n",
    "        shared_output = torch.zeros_like(hidden_states)\n",
    "        \n",
    "        for expert in self.shared_experts:\n",
    "            shared_output = shared_output + expert(hidden_states)\n",
    "        \n",
    "        # ================================================================\n",
    "        # 2. Routed Expert Processing\n",
    "        # ================================================================\n",
    "        # Get routing decisions\n",
    "        expert_indices, expert_weights, router_logits = self.router(hidden_states)\n",
    "        \n",
    "        # Flatten for processing\n",
    "        flat_hidden = hidden_states.view(-1, input_dim)  # [batch*seq, input_dim]\n",
    "        flat_indices = expert_indices.view(-1, self.top_k)  # [batch*seq, top_k]\n",
    "        flat_weights = expert_weights.view(-1, self.top_k)  # [batch*seq, top_k]\n",
    "        \n",
    "        # Initialize routed output\n",
    "        routed_output = torch.zeros_like(flat_hidden)\n",
    "        \n",
    "        # Process each expert\n",
    "        for expert_idx in range(self.num_routed_experts):\n",
    "            # Find tokens that selected this expert\n",
    "            expert_mask = (flat_indices == expert_idx)  # [batch*seq, top_k]\n",
    "            \n",
    "            if expert_mask.any():\n",
    "                # Get tokens for this expert\n",
    "                token_indices = expert_mask.any(dim=1).nonzero(as_tuple=True)[0]\n",
    "                \n",
    "                if len(token_indices) > 0:\n",
    "                    # Get inputs for this expert\n",
    "                    expert_input = flat_hidden[token_indices]\n",
    "                    \n",
    "                    # Process through expert\n",
    "                    expert_output = self.routed_experts[expert_idx](expert_input)\n",
    "                    \n",
    "                    # Get weights for tokens that selected this expert\n",
    "                    expert_token_weights = flat_weights[token_indices]\n",
    "                    expert_token_mask = expert_mask[token_indices]\n",
    "                    \n",
    "                    # Weight expert output\n",
    "                    weights = torch.where(\n",
    "                        expert_token_mask,\n",
    "                        expert_token_weights,\n",
    "                        torch.zeros_like(expert_token_weights)\n",
    "                    ).sum(dim=1, keepdim=True)\n",
    "                    \n",
    "                    # Add to routed output\n",
    "                    routed_output[token_indices] += expert_output * weights\n",
    "        \n",
    "        # Reshape routed output\n",
    "        routed_output = routed_output.view(batch_size, seq_len, input_dim)\n",
    "        \n",
    "        # ================================================================\n",
    "        # 3. Combine Shared and Routed Outputs\n",
    "        # ================================================================\n",
    "        output = shared_output + routed_output\n",
    "        \n",
    "        # ================================================================\n",
    "        # 4. Compute Auxiliary Losses (if training)\n",
    "        # ================================================================\n",
    "        aux_loss = None\n",
    "        if return_aux_loss and self.training:\n",
    "            aux_loss = self._compute_auxiliary_loss(router_logits, expert_indices)\n",
    "        \n",
    "        return output, aux_loss\n",
    "    \n",
    "    def _compute_auxiliary_loss(\n",
    "        self,\n",
    "        router_logits: torch.Tensor,\n",
    "        expert_indices: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute auxiliary losses for load balancing.\n",
    "        \n",
    "        Args:\n",
    "            router_logits: [batch, seq, num_experts]\n",
    "            expert_indices: [batch, seq, top_k]\n",
    "        \n",
    "        Returns:\n",
    "            aux_loss: Combined auxiliary loss\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, num_experts = router_logits.size()\n",
    "        \n",
    "        # ================================================================\n",
    "        # Load Balancing Loss\n",
    "        # ================================================================\n",
    "        # Compute fraction of tokens routed to each expert\n",
    "        expert_counts = torch.zeros(num_experts, device=router_logits.device)\n",
    "        \n",
    "        for i in range(num_experts):\n",
    "            expert_counts[i] = (expert_indices == i).sum().float()\n",
    "        \n",
    "        # Normalize\n",
    "        total_tokens = batch_size * seq_len * self.top_k\n",
    "        expert_fractions = expert_counts / total_tokens\n",
    "        \n",
    "        # Balance loss: encourages uniform distribution\n",
    "        balance_loss = num_experts * (expert_fractions ** 2).sum()\n",
    "        \n",
    "        # ================================================================\n",
    "        # Importance Loss\n",
    "        # ================================================================\n",
    "        # Compute importance (average gate value per expert)\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        importance = router_probs.sum(dim=[0, 1])  # [num_experts]\n",
    "        importance = importance / (batch_size * seq_len)\n",
    "        \n",
    "        # Importance loss: encourages diversity\n",
    "        importance_loss = (importance ** 2).sum() * num_experts\n",
    "        \n",
    "        # ================================================================\n",
    "        # Combined Loss\n",
    "        # ================================================================\n",
    "        aux_loss = (\n",
    "            self.aux_loss_alpha * balance_loss +\n",
    "            self.aux_loss_beta * importance_loss\n",
    "        )\n",
    "        \n",
    "        return aux_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ae823",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 4. DEEPSEEK-V3 MOE LAYER (AUXILIARY-LOSS-FREE)\n",
    "# ============================================================================\n",
    "\n",
    "class DeepSeekV3MoE(nn.Module):\n",
    "    \"\"\"\n",
    "    DeepSeek-V3 MoE layer with:\n",
    "    - More routed experts (256)\n",
    "    - Single shared expert\n",
    "    - More experts per token (K=8)\n",
    "    - Auxiliary-loss-free training (intrinsic balancing)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 7168,\n",
    "        num_routed_experts: int = 256,\n",
    "        expert_hidden_dim: int = 2048,\n",
    "        top_k: int = 8,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Model dimension\n",
    "            num_routed_experts: Number of routed experts\n",
    "            expert_hidden_dim: Hidden dimension for each expert\n",
    "            top_k: Number of experts to activate per token\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(DeepSeekV3MoE, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_routed_experts = num_routed_experts\n",
    "        self.expert_hidden_dim = expert_hidden_dim\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Router with bias (for intrinsic balancing)\n",
    "        self.router = TopKRouter(\n",
    "            input_dim=input_dim,\n",
    "            num_experts=num_routed_experts,\n",
    "            top_k=top_k,\n",
    "            use_bias=True  # V3 uses bias for balancing\n",
    "        )\n",
    "        \n",
    "        # Routed experts\n",
    "        self.routed_experts = nn.ModuleList([\n",
    "            SwiGLUExpert(input_dim, expert_hidden_dim, dropout)\n",
    "            for _ in range(num_routed_experts)\n",
    "        ])\n",
    "        \n",
    "        # Single shared expert\n",
    "        self.shared_expert = SwiGLUExpert(input_dim, expert_hidden_dim, dropout)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass (no auxiliary loss needed!).\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: [batch_size, seq_len, input_dim]\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch_size, seq_len, input_dim]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, input_dim = hidden_states.size()\n",
    "        \n",
    "        # ================================================================\n",
    "        # 1. Shared Expert Processing\n",
    "        # ================================================================\n",
    "        shared_output = self.shared_expert(hidden_states)\n",
    "        \n",
    "        # ================================================================\n",
    "        # 2. Routed Expert Processing\n",
    "        # ================================================================\n",
    "        expert_indices, expert_weights, _ = self.router(hidden_states)\n",
    "        \n",
    "        # Flatten\n",
    "        flat_hidden = hidden_states.view(-1, input_dim)\n",
    "        flat_indices = expert_indices.view(-1, self.top_k)\n",
    "        flat_weights = expert_weights.view(-1, self.top_k)\n",
    "        \n",
    "        # Initialize routed output\n",
    "        routed_output = torch.zeros_like(flat_hidden)\n",
    "        \n",
    "        # Process each expert\n",
    "        for expert_idx in range(self.num_routed_experts):\n",
    "            expert_mask = (flat_indices == expert_idx)\n",
    "            \n",
    "            if expert_mask.any():\n",
    "                token_indices = expert_mask.any(dim=1).nonzero(as_tuple=True)[0]\n",
    "                \n",
    "                if len(token_indices) > 0:\n",
    "                    expert_input = flat_hidden[token_indices]\n",
    "                    expert_output = self.routed_experts[expert_idx](expert_input)\n",
    "                    \n",
    "                    expert_token_weights = flat_weights[token_indices]\n",
    "                    expert_token_mask = expert_mask[token_indices]\n",
    "                    \n",
    "                    weights = torch.where(\n",
    "                        expert_token_mask,\n",
    "                        expert_token_weights,\n",
    "                        torch.zeros_like(expert_token_weights)\n",
    "                    ).sum(dim=1, keepdim=True)\n",
    "                    \n",
    "                    routed_output[token_indices] += expert_output * weights\n",
    "        \n",
    "        routed_output = routed_output.view(batch_size, seq_len, input_dim)\n",
    "        \n",
    "        # ================================================================\n",
    "        # 3. Combine (no auxiliary loss!)\n",
    "        # ================================================================\n",
    "        output = shared_output + routed_output\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9de1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. COMPLETE TRANSFORMER BLOCK WITH MOE\n",
    "# ============================================================================\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"RMS Layer Normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "\n",
    "class DeepSeekTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer block with MLA + MoE.\n",
    "    Simplified version using standard attention instead of MLA.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 5120,\n",
    "        num_attention_heads: int = 128,\n",
    "        moe_config: dict = None,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_attention_heads: Number of attention heads\n",
    "            moe_config: Configuration for MoE layer\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(DeepSeekTransformerBlock, self).__init__()\n",
    "        \n",
    "        # Layer norms\n",
    "        self.input_layernorm = RMSNorm(d_model)\n",
    "        self.post_attention_layernorm = RMSNorm(d_model)\n",
    "        \n",
    "        # Simplified: Use standard MHA (in practice, use MLA)\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model,\n",
    "            num_heads=num_attention_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # MoE layer\n",
    "        if moe_config is None:\n",
    "            moe_config = {\n",
    "                'type': 'v2',\n",
    "                'num_routed_experts': 64,\n",
    "                'num_shared_experts': 2,\n",
    "                'expert_hidden_dim': 1536,\n",
    "                'top_k': 6\n",
    "            }\n",
    "        \n",
    "        if moe_config['type'] == 'v2':\n",
    "            self.mlp = DeepSeekV2MoE(\n",
    "                input_dim=d_model,\n",
    "                num_routed_experts=moe_config['num_routed_experts'],\n",
    "                num_shared_experts=moe_config['num_shared_experts'],\n",
    "                expert_hidden_dim=moe_config['expert_hidden_dim'],\n",
    "                top_k=moe_config['top_k'],\n",
    "                dropout=dropout\n",
    "            )\n",
    "        else:  # v3\n",
    "            self.mlp = DeepSeekV3MoE(\n",
    "                input_dim=d_model,\n",
    "                num_routed_experts=moe_config['num_routed_experts'],\n",
    "                expert_hidden_dim=moe_config['expert_hidden_dim'],\n",
    "                top_k=moe_config['top_k'],\n",
    "                dropout=dropout\n",
    "            )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        return_aux_loss: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            hidden_states: [batch, seq_len, d_model]\n",
    "            attention_mask: Optional attention mask\n",
    "            return_aux_loss: Whether to return auxiliary loss\n",
    "        \n",
    "        Returns:\n",
    "            output: [batch, seq_len, d_model]\n",
    "            aux_loss: Auxiliary loss (if applicable)\n",
    "        \"\"\"\n",
    "        residual = hidden_states\n",
    "        \n",
    "        # Attention\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        attn_output, _ = self.self_attn(\n",
    "            hidden_states,\n",
    "            hidden_states,\n",
    "            hidden_states,\n",
    "            attn_mask=attention_mask,\n",
    "            need_weights=False\n",
    "        )\n",
    "        hidden_states = residual + attn_output\n",
    "        \n",
    "        # MoE\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        \n",
    "        if isinstance(self.mlp, DeepSeekV2MoE):\n",
    "            mlp_output, aux_loss = self.mlp(hidden_states, return_aux_loss)\n",
    "        else:  # V3\n",
    "            mlp_output = self.mlp(hidden_states)\n",
    "            aux_loss = None\n",
    "        \n",
    "        hidden_states = residual + mlp_output\n",
    "        \n",
    "        return hidden_states, aux_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d925cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 6. ANALYSIS AND BENCHMARKING\n",
    "# ============================================================================\n",
    "\n",
    "class MoEAnalyzer:\n",
    "    \"\"\"Analyze MoE behavior and statistics.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_expert_utilization(\n",
    "        expert_indices: torch.Tensor,\n",
    "        num_experts: int\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Analyze how experts are being utilized.\n",
    "        \n",
    "        Args:\n",
    "            expert_indices: [batch, seq_len, top_k]\n",
    "            num_experts: Total number of experts\n",
    "        \n",
    "        Returns:\n",
    "            statistics: Dictionary with utilization stats\n",
    "        \"\"\"\n",
    "        # Count expert selections\n",
    "        expert_counts = torch.zeros(num_experts)\n",
    "        \n",
    "        for i in range(num_experts):\n",
    "            expert_counts[i] = (expert_indices == i).sum().item()\n",
    "        \n",
    "        total_selections = expert_indices.numel()\n",
    "        expert_fractions = expert_counts / total_selections\n",
    "        \n",
    "        return {\n",
    "            'expert_counts': expert_counts.tolist(),\n",
    "            'expert_fractions': expert_fractions.tolist(),\n",
    "            'mean_fraction': expert_fractions.mean().item(),\n",
    "            'std_fraction': expert_fractions.std().item(),\n",
    "            'max_fraction': expert_fractions.max().item(),\n",
    "            'min_fraction': expert_fractions.min().item(),\n",
    "            'coefficient_of_variation': (expert_fractions.std() / expert_fractions.mean()).item()\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_parameters(model: nn.Module) -> dict:\n",
    "        \"\"\"Count parameters in MoE model.\"\"\"\n",
    "        total = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        # Try to separate routed and shared\n",
    "        routed = 0\n",
    "        shared = 0\n",
    "        \n",
    "        if hasattr(model, 'routed_experts'):\n",
    "            for expert in model.routed_experts:\n",
    "                routed += sum(p.numel() for p in expert.parameters())\n",
    "        \n",
    "        if hasattr(model, 'shared_experts'):\n",
    "            for expert in model.shared_experts:\n",
    "                shared += sum(p.numel() for p in expert.parameters())\n",
    "        elif hasattr(model, 'shared_expert'):\n",
    "            shared = sum(p.numel() for p in model.shared_expert.parameters())\n",
    "        \n",
    "        other = total - routed - shared\n",
    "        \n",
    "        return {\n",
    "            'total': total,\n",
    "            'routed_experts': routed,\n",
    "            'shared_experts': shared,\n",
    "            'other': other\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39796d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# 7. DEMONSTRATION EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "def demo_v2_moe():\n",
    "    \"\"\"Demonstrate DeepSeek-V2 MoE.\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"DEMO 1: DeepSeek-V2 MoE Layer\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 256  # Smaller for demo\n",
    "    \n",
    "    # Create V2 MoE\n",
    "    moe = DeepSeekV2MoE(\n",
    "        input_dim=d_model,\n",
    "        num_routed_experts=16,  # Smaller for demo\n",
    "        num_shared_experts=2,\n",
    "        expert_hidden_dim=128,\n",
    "        top_k=4\n",
    "    )\n",
    "    \n",
    "    # Input\n",
    "    hidden_states = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Forward\n",
    "    output, aux_loss = moe(hidden_states)\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Input dim: {d_model}\")\n",
    "    print(f\"  Routed experts: 16\")\n",
    "    print(f\"  Shared experts: 2\")\n",
    "    print(f\"  Top-K: 4\")\n",
    "    \n",
    "    print(f\"\\nShapes:\")\n",
    "    print(f\"  Input: {hidden_states.shape}\")\n",
    "    print(f\"  Output: {output.shape}\")\n",
    "    \n",
    "    print(f\"\\nAuxiliary Loss: {aux_loss.item():.6f}\")\n",
    "    \n",
    "    # Parameter count\n",
    "    params = MoEAnalyzer.count_parameters(moe)\n",
    "    print(f\"\\nParameters:\")\n",
    "    print(f\"  Total: {params['total']:,}\")\n",
    "    print(f\"  Routed experts: {params['routed_experts']:,}\")\n",
    "    print(f\"  Shared experts: {params['shared_experts']:,}\")\n",
    "    print(f\"  Router: {params['other']:,}\")\n",
    "\n",
    "\n",
    "def demo_v3_moe():\n",
    "    \"\"\"Demonstrate DeepSeek-V3 MoE.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEMO 2: DeepSeek-V3 MoE Layer (Auxiliary-Loss-Free)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 256\n",
    "    \n",
    "    # Create V3 MoE\n",
    "    moe = DeepSeekV3MoE(\n",
    "        input_dim=d_model,\n",
    "        num_routed_experts=32,  # Smaller for demo\n",
    "        expert_hidden_dim=128,\n",
    "        top_k=6\n",
    "    )\n",
    "    \n",
    "    # Input\n",
    "    hidden_states = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Forward (no auxiliary loss!)\n",
    "    output = moe(hidden_states)\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Input dim: {d_model}\")\n",
    "    print(f\"  Routed experts: 32\")\n",
    "    print(f\"  Shared experts: 1\")\n",
    "    print(f\"  Top-K: 6\")\n",
    "    \n",
    "    print(f\"\\nKey Innovation: No auxiliary loss needed!\")\n",
    "    print(f\"  Intrinsic balancing through:\")\n",
    "    print(f\"    - Router bias\")\n",
    "    print(f\"    - More experts\")\n",
    "    print(f\"    - Higher K\")\n",
    "    \n",
    "    params = MoEAnalyzer.count_parameters(moe)\n",
    "    print(f\"\\nParameters:\")\n",
    "    print(f\"  Total: {params['total']:,}\")\n",
    "\n",
    "\n",
    "def demo_expert_utilization():\n",
    "    \"\"\"Demonstrate expert utilization analysis.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEMO 3: Expert Utilization Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    batch_size = 8\n",
    "    seq_len = 100\n",
    "    d_model = 256\n",
    "    num_experts = 16\n",
    "    \n",
    "    # Create MoE\n",
    "    moe = DeepSeekV2MoE(\n",
    "        input_dim=d_model,\n",
    "        num_routed_experts=num_experts,\n",
    "        num_shared_experts=2,\n",
    "        expert_hidden_dim=128,\n",
    "        top_k=4\n",
    "    )\n",
    "    \n",
    "    # Generate data\n",
    "    hidden_states = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Get routing decisions\n",
    "    expert_indices, expert_weights, _ = moe.router(hidden_states)\n",
    "    \n",
    "    # Analyze\n",
    "    stats = MoEAnalyzer.analyze_expert_utilization(\n",
    "        expert_indices,\n",
    "        num_experts\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nExpert Utilization Statistics:\")\n",
    "    print(f\"  Mean usage: {stats['mean_fraction']:.4f}\")\n",
    "    print(f\"  Std deviation: {stats['std_fraction']:.4f}\")\n",
    "    print(f\"  Coefficient of variation: {stats['coefficient_of_variation']:.4f}\")\n",
    "    print(f\"  Max usage: {stats['max_fraction']:.4f}\")\n",
    "    print(f\"  Min usage: {stats['min_fraction']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPer-Expert Usage:\")\n",
    "    for i, frac in enumerate(stats['expert_fractions']):\n",
    "        bar = '█' * int(frac * 100)\n",
    "        print(f\"  Expert {i:2d}: {frac:.4f} {bar}\")\n",
    "\n",
    "\n",
    "def demo_comparison():\n",
    "    \"\"\"Compare V2 and V3.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEMO 4: V2 vs V3 Comparison\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    d_model = 512\n",
    "    \n",
    "    # V2 Configuration\n",
    "    v2_config = {\n",
    "        'num_routed_experts': 64,\n",
    "        'num_shared_experts': 2,\n",
    "        'expert_hidden_dim': 256,\n",
    "        'top_k': 6\n",
    "    }\n",
    "    \n",
    "    v2_moe = DeepSeekV2MoE(input_dim=d_model, **v2_config)\n",
    "    v2_params = MoEAnalyzer.count_parameters(v2_moe)\n",
    "    \n",
    "    # V3 Configuration\n",
    "    v3_config = {\n",
    "        'num_routed_experts': 128,\n",
    "        'expert_hidden_dim': 256,\n",
    "        'top_k': 8\n",
    "    }\n",
    "    \n",
    "    v3_moe = DeepSeekV3MoE(input_dim=d_model, **v3_config)\n",
    "    v3_params = MoEAnalyzer.count_parameters(v3_moe)\n",
    "    \n",
    "    print(\"\\nDeepSeek-V2 MoE:\")\n",
    "    print(f\"  Routed experts: {v2_config['num_routed_experts']}\")\n",
    "    print(f\"  Shared experts: {v2_config['num_shared_experts']}\")\n",
    "    print(f\"  Top-K: {v2_config['top_k']}\")\n",
    "    print(f\"  Total params: {v2_params['total']:,}\")\n",
    "    print(f\"  Has auxiliary loss: Yes\")\n",
    "    \n",
    "    print(\"\\nDeepSeek-V3 MoE:\")\n",
    "    print(f\"  Routed experts: {v3_config['num_routed_experts']}\")\n",
    "    print(f\"  Shared experts: 1\")\n",
    "    print(f\"  Top-K: {v3_config['top_k']}\")\n",
    "    print(f\"  Total params: {v3_params['total']:,}\")\n",
    "    print(f\"  Has auxiliary loss: No (intrinsic balancing)\")\n",
    "    \n",
    "    # Active parameters\n",
    "    v2_active = (v2_config['top_k'] + v2_config['num_shared_experts']) * v2_params['routed_experts'] / v2_config['num_routed_experts']\n",
    "    v3_active = (v3_config['top_k'] + 1) * v3_params['routed_experts'] / v3_config['num_routed_experts']\n",
    "    \n",
    "    print(f\"\\nActive Parameters per Token:\")\n",
    "    print(f\"  V2: ~{v2_active/1e6:.1f}M\")\n",
    "    print(f\"  V3: ~{v3_active/1e6:.1f}M\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75137f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DEEPSEEK MIXTURE OF EXPERTS: COMPLETE IMPLEMENTATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Run all demos\n",
    "    demo_v2_moe()\n",
    "    demo_v3_moe()\n",
    "    demo_expert_utilization()\n",
    "    demo_comparison()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"All demonstrations completed successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nKey Takeaways:\")\n",
    "    print(\"1. Fine-grained experts (small, specialized)\")\n",
    "    print(\"2. Shared experts (always active, common knowledge)\")\n",
    "    print(\"3. V2: Auxiliary losses for balancing\")\n",
    "    print(\"4. V3: Auxiliary-loss-free with intrinsic balancing\")\n",
    "    print(\"5. Dramatic parameter efficiency gains\")\n",
    "    print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
